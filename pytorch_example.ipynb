{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-03-02T19:47:26.923225700Z",
     "start_time": "2024-03-02T19:47:26.915819900Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7071066904050358\n",
      "---\n",
      "x2 0.5000001283844369\n",
      "w2 0.0\n",
      "x1 -1.5000003851533106\n",
      "w1 1.0000002567688737\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import math\n",
    "from Value import Value\n",
    "\n",
    "x1 = torch.Tensor([2.0]).double()                ; x1.requires_grad = True\n",
    "x2 = torch.Tensor([0.0]).double()                ; x2.requires_grad = True\n",
    "w1 = torch.Tensor([-3.0]).double()               ; w1.requires_grad = True\n",
    "w2 = torch.Tensor([1.0]).double()                ; w2.requires_grad = True\n",
    "b = torch.Tensor([6.8813735870195432]).double()  ; b.requires_grad = True\n",
    "n = x1*w1 + x2*w2 + b\n",
    "o = torch.tanh(n)\n",
    "\n",
    "print(o.data.item())\n",
    "o.backward()\n",
    "\n",
    "print('---')\n",
    "print('x2', x2.grad.item())\n",
    "print('w2', w2.grad.item())\n",
    "print('x1', x1.grad.item())\n",
    "print('w1', w1.grad.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "\n",
    "class Neuron:\n",
    "\n",
    "  def __init__(self, nin):\n",
    "    self.w = [Value(random.uniform(-1,1)) for _ in range(nin)]\n",
    "    self.b = Value(random.uniform(-1,1))\n",
    "\n",
    "  def __call__(self, x):\n",
    "    # w * x + b\n",
    "    act = sum((wi*xi for wi, xi in zip(self.w, x)), self.b)\n",
    "    out = act.tanh()\n",
    "    return out\n",
    "\n",
    "  def parameters(self):\n",
    "    return self.w + [self.b]\n",
    "\n",
    "class Layer:\n",
    "\n",
    "  def __init__(self, nin, nout):\n",
    "    self.neurons = [Neuron(nin) for _ in range(nout)]\n",
    "\n",
    "  def __call__(self, x):\n",
    "    outs = [n(x) for n in self.neurons]\n",
    "    return outs[0] if len(outs) == 1 else outs\n",
    "\n",
    "  def parameters(self):\n",
    "    return [p for neuron in self.neurons for p in neuron.parameters()]\n",
    "\n",
    "class MLP:\n",
    "\n",
    "  def __init__(self, nin, nouts):\n",
    "    sz = [nin] + nouts\n",
    "    self.layers = [Layer(sz[i], sz[i+1]) for i in range(len(nouts))]\n",
    "\n",
    "  def __call__(self, x):\n",
    "    for layer in self.layers:\n",
    "      x = layer(x)\n",
    "    return x\n",
    "\n",
    "  def parameters(self):\n",
    "    return [p for layer in self.layers for p in layer.parameters()]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-02T19:47:26.939929Z",
     "start_time": "2024-03-02T19:47:26.928232Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'math' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[38], line 3\u001B[0m\n\u001B[0;32m      1\u001B[0m x \u001B[38;5;241m=\u001B[39m [\u001B[38;5;241m2.0\u001B[39m, \u001B[38;5;241m3.0\u001B[39m, \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1.0\u001B[39m]\n\u001B[0;32m      2\u001B[0m n \u001B[38;5;241m=\u001B[39m MLP(\u001B[38;5;241m3\u001B[39m, [\u001B[38;5;241m4\u001B[39m, \u001B[38;5;241m4\u001B[39m, \u001B[38;5;241m1\u001B[39m])\n\u001B[1;32m----> 3\u001B[0m \u001B[43mn\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[1;32mIn[37], line 39\u001B[0m, in \u001B[0;36mMLP.__call__\u001B[1;34m(self, x)\u001B[0m\n\u001B[0;32m     37\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__call__\u001B[39m(\u001B[38;5;28mself\u001B[39m, x):\n\u001B[0;32m     38\u001B[0m   \u001B[38;5;28;01mfor\u001B[39;00m layer \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlayers:\n\u001B[1;32m---> 39\u001B[0m     x \u001B[38;5;241m=\u001B[39m \u001B[43mlayer\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     40\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m x\n",
      "Cell \u001B[1;32mIn[37], line 25\u001B[0m, in \u001B[0;36mLayer.__call__\u001B[1;34m(self, x)\u001B[0m\n\u001B[0;32m     24\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__call__\u001B[39m(\u001B[38;5;28mself\u001B[39m, x):\n\u001B[1;32m---> 25\u001B[0m   outs \u001B[38;5;241m=\u001B[39m [\u001B[43mn\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m \u001B[38;5;28;01mfor\u001B[39;00m n \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mneurons]\n\u001B[0;32m     26\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m outs[\u001B[38;5;241m0\u001B[39m] \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(outs) \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m1\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m outs\n",
      "Cell \u001B[1;32mIn[37], line 13\u001B[0m, in \u001B[0;36mNeuron.__call__\u001B[1;34m(self, x)\u001B[0m\n\u001B[0;32m     10\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__call__\u001B[39m(\u001B[38;5;28mself\u001B[39m, x):\n\u001B[0;32m     11\u001B[0m   \u001B[38;5;66;03m# w * x + b\u001B[39;00m\n\u001B[0;32m     12\u001B[0m   act \u001B[38;5;241m=\u001B[39m \u001B[38;5;28msum\u001B[39m((wi\u001B[38;5;241m*\u001B[39mxi \u001B[38;5;28;01mfor\u001B[39;00m wi, xi \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mzip\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mw, x)), \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mb)\n\u001B[1;32m---> 13\u001B[0m   out \u001B[38;5;241m=\u001B[39m \u001B[43mact\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtanh\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     14\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m out\n",
      "File \u001B[1;32m~\\coding_stuff\\PycharmProjects\\my_micrograd\\Value.py:64\u001B[0m, in \u001B[0;36mValue.tanh\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m     62\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mtanh\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[0;32m     63\u001B[0m   x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdata\n\u001B[1;32m---> 64\u001B[0m   t \u001B[38;5;241m=\u001B[39m (\u001B[43mmath\u001B[49m\u001B[38;5;241m.\u001B[39mexp(\u001B[38;5;241m2\u001B[39m\u001B[38;5;241m*\u001B[39mx) \u001B[38;5;241m-\u001B[39m \u001B[38;5;241m1\u001B[39m)\u001B[38;5;241m/\u001B[39m(math\u001B[38;5;241m.\u001B[39mexp(\u001B[38;5;241m2\u001B[39m\u001B[38;5;241m*\u001B[39mx) \u001B[38;5;241m+\u001B[39m \u001B[38;5;241m1\u001B[39m)\n\u001B[0;32m     65\u001B[0m   out \u001B[38;5;241m=\u001B[39m Value(t, (\u001B[38;5;28mself\u001B[39m, ), \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtanh\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m     67\u001B[0m   \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_backward\u001B[39m():\n",
      "\u001B[1;31mNameError\u001B[0m: name 'math' is not defined"
     ]
    }
   ],
   "source": [
    "x = [2.0, 3.0, -1.0]\n",
    "n = MLP(3, [4, 4, 1])\n",
    "n(x)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-02T19:47:26.976739Z",
     "start_time": "2024-03-02T19:47:26.935416400Z"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
